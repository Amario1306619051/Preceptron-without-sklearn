{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Handling\n",
    "\n",
    "The provided code snippet demonstrates how to load and handle data using the Pandas library in Python. It begins by importing the Pandas library and then reads a CSV file named 'datak.csv' into a DataFrame called 'df'. The DataFrame is initially loaded without considering any specific values as missing, but the second line of code shows how to specify specific values ('na', '-', 'NaN') as NaN (Not a Number) values during the loading process, allowing for better data preprocessing and handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datak.csv')\n",
    "df = pd.read_csv('datak.csv',na_values = [\"na\",\"-\",\"NaN\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nama</th>\n",
       "      <th>pac</th>\n",
       "      <th>sho</th>\n",
       "      <th>pas</th>\n",
       "      <th>dri</th>\n",
       "      <th>def</th>\n",
       "      <th>phy</th>\n",
       "      <th>lbl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kevin De Bruyne</td>\n",
       "      <td>76</td>\n",
       "      <td>86</td>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "      <td>64</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ederson</td>\n",
       "      <td>87</td>\n",
       "      <td>82</td>\n",
       "      <td>93</td>\n",
       "      <td>88</td>\n",
       "      <td>64</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Raheem Sterling</td>\n",
       "      <td>91</td>\n",
       "      <td>82</td>\n",
       "      <td>79</td>\n",
       "      <td>87</td>\n",
       "      <td>45</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ruben Dias</td>\n",
       "      <td>61</td>\n",
       "      <td>38</td>\n",
       "      <td>65</td>\n",
       "      <td>68</td>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Joao Cancelo</td>\n",
       "      <td>85</td>\n",
       "      <td>71</td>\n",
       "      <td>83</td>\n",
       "      <td>84</td>\n",
       "      <td>80</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Nama  pac  sho  pas  dri  def  phy  lbl\n",
       "0  Kevin De Bruyne   76   86   93   88   64   78    0\n",
       "1          Ederson   87   82   93   88   64   88    0\n",
       "2  Raheem Sterling   91   82   79   87   45   66    1\n",
       "3       Ruben Dias   61   38   65   68   88   88    0\n",
       "4     Joao Cancelo   85   71   83   84   80   72    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIFA 2021 Player Performance Dataset\n",
    "\n",
    "The provided dataset contains performance statistics of football (soccer) players featured in the FIFA 2021 video game. Each row represents a player, and the columns provide information about the player's abilities in various aspects of the game.\n",
    "\n",
    "![Card](./mbappe.jpeg)\n",
    "\n",
    "### Columns Explanation:\n",
    "- **Name**: Name of the player.\n",
    "- **pac**: Pace statistic of the player.\n",
    "- **sho**: Shooting skill statistic of the player.\n",
    "- **pas**: Passing skill statistic of the player.\n",
    "- **dri**: Dribbling skill statistic of the player.\n",
    "- **def**: Defending skill statistic of the player.\n",
    "- **phy**: Physical attribute statistic of the player.\n",
    "- **lbl**: Label indicating whether the player is a forward (1) or not a forward (0) in the game.\n",
    "\n",
    "This dataset offers insights into different attributes and skills of each player, such as speed, shooting accuracy, passing ability, dribbling proficiency, defensive capabilities, physical attributes, and their designated role or position as either a forward or not a forward in the FIFA 2021 game.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction and Normalization\n",
    "\n",
    "In this section, we extract the features from the dataset and normalize them for further analysis.\n",
    "\n",
    "```python\n",
    "# List of features to be used in analysis\n",
    "features = ['pac', 'sho', 'pas', 'dri', 'def', 'phy']\n",
    "\n",
    "# Retrieve feature values from data as a numpy array\n",
    "data_features = df.loc[:, features].values\n",
    "\n",
    "# Display the dimension of feature data\n",
    "n = len(data_features)\n",
    "print(\"Number of data points:\", n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 28\n",
      "Normalized Feature Data:\n",
      "[[0.76 0.86 0.93 0.88 0.64 0.78]\n",
      " [0.87 0.82 0.93 0.88 0.64 0.88]\n",
      " [0.91 0.82 0.79 0.87 0.45 0.66]\n",
      " [0.61 0.38 0.65 0.68 0.88 0.88]\n",
      " [0.85 0.71 0.83 0.84 0.8  0.72]\n",
      " [0.65 0.8  0.85 0.86 0.73 0.72]\n",
      " [0.92 0.63 0.76 0.78 0.8  0.82]\n",
      " [0.8  0.76 0.83 0.88 0.46 0.64]\n",
      " [0.84 0.78 0.8  0.87 0.56 0.57]\n",
      " [0.59 0.72 0.75 0.78 0.84 0.75]\n",
      " [0.81 0.79 0.81 0.9  0.38 0.6 ]\n",
      " [0.89 0.91 0.65 0.8  0.45 0.88]\n",
      " [0.71 0.89 0.75 0.87 0.33 0.69]\n",
      " [0.84 0.86 0.86 0.9  0.4  0.6 ]\n",
      " [0.83 0.81 0.86 0.87 0.48 0.69]\n",
      " [0.86 0.87 0.67 0.81 0.39 0.77]\n",
      " [0.87 0.78 0.85 0.9  0.36 0.45]\n",
      " [0.7  0.73 0.75 0.85 0.65 0.66]\n",
      " [0.63 0.51 0.73 0.72 0.81 0.78]\n",
      " [0.65 0.31 0.65 0.68 0.83 0.72]\n",
      " [0.81 0.74 0.77 0.82 0.67 0.74]\n",
      " [0.58 0.74 0.78 0.77 0.75 0.77]\n",
      " [0.56 0.7  0.81 0.77 0.77 0.75]\n",
      " [0.76 0.79 0.75 0.8  0.58 0.68]\n",
      " [0.78 0.77 0.85 0.81 0.34 0.76]\n",
      " [0.5  0.64 0.79 0.69 0.69 0.8 ]\n",
      " [0.36 0.67 0.75 0.73 0.8  0.73]\n",
      " [0.73 0.42 0.58 0.67 0.79 0.77]]\n"
     ]
    }
   ],
   "source": [
    "# List of features to be used in analysis\n",
    "features = ['pac', 'sho', 'pas', 'dri', 'def', 'phy']\n",
    "\n",
    "# Retrieve feature values from data as a numpy array\n",
    "data_features = df.loc[:, features].values\n",
    "\n",
    "# Display the dimension of feature data\n",
    "n = len(data_features)\n",
    "print(\"Number of data points:\", n)\n",
    "\n",
    "# Normalize feature data by dividing each value by 100\n",
    "data_normalized = data_features[:, :] / 100\n",
    "\n",
    "# Display normalized feature data\n",
    "print(\"Normalized Feature Data:\")\n",
    "print(data_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! Here's the provided code converted to markdown format in a Jupyter Notebook style with brief descriptions:\n",
    "\n",
    "```markdown\n",
    "### Target Data and Dimensionality\n",
    "\n",
    "In this section, we explore the dimensionality of the feature data and retrieve the target values for analysis.\n",
    "\n",
    "```python\n",
    "# Number of dimensions in feature data\n",
    "n_dimensions = len(data_normalized[0])\n",
    "print(\"Number of Feature Dimensions:\", n_dimensions)\n",
    "```\n",
    "\n",
    "Here, we calculate the number of dimensions present in the normalized feature data. This will help us understand the complexity of the feature space.\n",
    "\n",
    "```python\n",
    "# Target column name\n",
    "target_column = 'lbl'\n",
    "\n",
    "# Retrieve target values from data as a numpy array\n",
    "data_target = df.loc[:, target_column].values\n",
    "\n",
    "# Display target values\n",
    "print(\"Target Data:\")\n",
    "print(data_target)\n",
    "```\n",
    "\n",
    "We identify the column name that represents the target variable in our dataset. Then, we retrieve the target values from the dataset as a numpy array and display them. The target values typically represent the labels or outcomes that we want to predict or analyze.\n",
    "\n",
    "This concludes the step where we explore the dimensionality of the feature data and obtain the target values for analysis.\n",
    "```\n",
    "\n",
    "Feel free to include this markdown content in your Jupyter Notebook to provide explanations for the code snippet you've shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Feature Dimensions: 6\n",
      "Target Data:\n",
      "[0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Number of dimensions in feature data\n",
    "n_dimensions = len(data_normalized[0])\n",
    "print(\"Number of Feature Dimensions:\", n_dimensions)\n",
    "\n",
    "# Target column name\n",
    "target_column = 'lbl'\n",
    "\n",
    "# Retrieve target values from data as a numpy array\n",
    "data_target = df.loc[:, target_column].values\n",
    "\n",
    "# Display target values\n",
    "print(\"Target Data:\")\n",
    "print(data_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Perceptron Learning Algorithm\n",
    "\n",
    "In this section, we perform the training of a perceptron model using the provided data. The perceptron learning algorithm adjusts the weights to minimize prediction errors.\n",
    "\n",
    "```python\n",
    "# Learning rate value\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Initialize initial weights\n",
    "weights = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Threshold value\n",
    "threshold = 0\n",
    "\n",
    "# List to store errors\n",
    "error_list = []\n",
    "\n",
    "# Model training\n",
    "for i in range(n):\n",
    "    V = 0\n",
    "    for k in range(n_dimensions):\n",
    "        V += data_normalized[i][k] * weights[k]\n",
    "    if V < threshold:\n",
    "        prediction = 0\n",
    "    else:\n",
    "        prediction = 1\n",
    "    error = data_target[i] - prediction\n",
    "    error_list.append(error)\n",
    "    for k in range(n_dimensions):\n",
    "        weights[k] += learning_rate * error * data_normalized[i][k]\n",
    "```\n",
    "\n",
    "Here, we set the learning rate, initialize the initial weights, and define the threshold value. We then proceed to perform the model training using the perceptron learning algorithm.\n",
    "\n",
    "In each iteration, the algorithm computes the weighted sum `V` of the normalized feature values multiplied by the corresponding weights. If the computed `V` is less than the threshold, the prediction is set to 0; otherwise, it is set to 1. The error is calculated as the difference between the target value and the prediction.\n",
    "\n",
    "The algorithm updates the weights for each feature dimension based on the learning rate and the error term.\n",
    "\n",
    "A list `error_list` is used to store the errors for analysis and convergence monitoring.\n",
    "\n",
    "This completes the training phase of the perceptron model using the provided data.\n",
    "```\n",
    "\n",
    "Feel free to include this markdown content in your Jupyter Notebook to provide explanations for the code snippet you've shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate value\n",
    "learning_rate = 0.9\n",
    "\n",
    "# Initialize initial weights\n",
    "weights = [1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Threshold value\n",
    "threshold = 0\n",
    "\n",
    "# List to store errors\n",
    "error_list = []\n",
    "\n",
    "# Model training\n",
    "for i in range(n):\n",
    "    V = 0\n",
    "    for k in range(n_dimensions):\n",
    "        V += data_normalized[i][k] * weights[k]\n",
    "    if V < threshold:\n",
    "        prediction = 0\n",
    "    else:\n",
    "        prediction = 1\n",
    "    error = data_target[i] - prediction\n",
    "    error_list.append(error)\n",
    "    for k in range(n_dimensions):\n",
    "        weights[k] += learning_rate * error * data_normalized[i][k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Model Refinement: Epochs and Convergence\n",
    "\n",
    "In this section, we refine the trained perceptron model iteratively using the concept of epochs. Each epoch consists of updating the model weights and checking for convergence.\n",
    "\n",
    "```python\n",
    "# Iterative model refinement using epochs\n",
    "previous_errors = []\n",
    "loop = 0\n",
    "while error_list != previous_errors:\n",
    "    previous_errors = error_list.copy()\n",
    "    error_list = []\n",
    "    for i in range(n):\n",
    "        V = 0\n",
    "        for k in range(n_dimensions):\n",
    "            V += data_normalized[i][k] * weights[k]\n",
    "        if V < threshold:\n",
    "            prediction = 0\n",
    "        else:\n",
    "            prediction = 1\n",
    "        error = data_target[i] - prediction\n",
    "        error_list.append(error)\n",
    "        for k in range(n_dimensions):\n",
    "            weights[k] += learning_rate * error * data_normalized[i][k]\n",
    "    loop += 1\n",
    "    print(\"Epoch:\", loop)\n",
    "```\n",
    "\n",
    "Here, we perform iterative model refinement using the concept of epochs. An epoch represents one complete iteration through the entire dataset.\n",
    "\n",
    "In each epoch, we update the model weights based on the prediction errors for each data point. We calculate the weighted sum `V` of the normalized feature values multiplied by the corresponding weights. If `V` is less than the threshold, the prediction is set to 0; otherwise, it is set to 1. The error is then computed as the difference between the target value and the prediction.\n",
    "\n",
    "The algorithm updates the weights for each feature dimension based on the learning rate and the error term.\n",
    "\n",
    "The loop continues until the error list remains the same in consecutive epochs, indicating convergence.\n",
    "\n",
    "You can customize the number of epochs based on the desired convergence criteria. The loop will continue until the error list no longer changes significantly, reflecting the training process's convergence.\n",
    "\n",
    "This concludes the iterative refinement of the perceptron model using the concept of epochs and convergence.\n",
    "```\n",
    "\n",
    "Feel free to include this markdown content in your Jupyter Notebook to provide explanations for the code snippet you've shared, including the concept of epochs and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop: 1\n",
      "Loop: 2\n",
      "Loop: 3\n"
     ]
    }
   ],
   "source": [
    "# Iterative model refinement\n",
    "previous_errors = []\n",
    "loop = 0\n",
    "while error_list != previous_errors:\n",
    "    previous_errors = error_list.copy()\n",
    "    error_list = []\n",
    "    for i in range(n):\n",
    "        V = 0\n",
    "        for k in range(n_dimensions):\n",
    "            V += data_normalized[i][k] * weights[k]\n",
    "        if V < threshold:\n",
    "            prediction = 0\n",
    "        else:\n",
    "            prediction = 1\n",
    "        error = data_target[i] - prediction\n",
    "        error_list.append(error)\n",
    "        for k in range(n_dimensions):\n",
    "            weights[k] += learning_rate * error * data_normalized[i][k]\n",
    "    loop += 1\n",
    "    print(\"Loop:\", loop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data for testing\n",
    "dff = pd.read_csv('datak1.csv', na_values=[\"na\", \"-\", \"NaN\"])\n",
    "\n",
    "# Retrieve feature values from testing data as a numpy array\n",
    "dff_features = dff.loc[:, features].values\n",
    "\n",
    "# Normalize testing feature data by dividing each value by 100\n",
    "dff_normalized = dff_features[:, :] / 100\n",
    "\n",
    "# Retrieve target values from testing data as a numpy array\n",
    "dff_target = dff.loc[:, target_column].values\n",
    "\n",
    "# List to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Model testing\n",
    "for i in range(len(dff)):\n",
    "    V = 0\n",
    "    for k in range(n_dimensions):\n",
    "        V += dff_normalized[i][k] * weights[k]\n",
    "    if V < threshold:\n",
    "        prediction = 0\n",
    "    else:\n",
    "        prediction = 1\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Calculate accuracy\n",
    "correct_predictions = sum(1 for i in range(len(dff)) if predictions[i] == dff_target[i])\n",
    "accuracy = (correct_predictions / len(dff)) * 100\n",
    "print('Accuracy of the data is:', accuracy, 'percent')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
